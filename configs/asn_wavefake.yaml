# audioShieldNet/asnet_6/configs/asn_wavefake_split.yaml
# ============================================================
# ASNet on WaveFake â€” deterministic loaders + optional auto-prepare
# Balanced batches supported (set data.sampler: balanced_batch)
# ============================================================

ckpt_dir: wavefake_cmra_energy_1

log:
  outdir: audioShieldNet/asnet_6/experiments/${ckpt_dir}
  wandb:
    enable: true
    project: audioshieldnet
    entity: xxxxxxxxxxxx
    run_name: ${ckpt_dir}

# -------------------------------
# DATA
# -------------------------------
data:
  name: wavefake
  sr: 16000
  n_fft: 1024
  hop: 256
  n_mels: 80
  max_secs: 6.0

  root_dir: /scratch/xxxxx/projects/deepfake/dataset/audio/waveFake

  # If you already have CSVs, keep auto_prepare: false and set explicit CSV paths.
  auto_prepare: false
  train_csv: /scratch/xxxxx/projects/deepfake/dataset/audio/waveFake/combined/WaveFake_train.csv
  val_csv:   /scratch/xxxxx/projects/deepfake/dataset/audio/waveFake/combined/WaveFake_val.csv
  test_csv:  /scratch/xxxxx/projects/deepfake/dataset/audio/waveFake/combined/WaveFake_test.csv

  # To build CSVs automatically, flip to true and (optionally) point to real/fake roots:
  # auto_prepare: true
  # fake_roots: []   # default: <root>/generated_audio/*
  # real_roots: [ /scratch/xxxxx/voice_cloning/datasets/LJSpeech/LJSpeech-1.1/wavs ]
  # split: {train: 0.8, val: 0.1, test: 0.1}

  # Sampling strategy: "uniform" | "weighted" | "balanced_batch"
  sampler: uniform
  # balanced_batch:
  #   real_per_batch: 128
  #   fake_per_batch: 128

  # Optional CAL from VAL (stratified); avoids touching TEST
  # cal_from_val_frac: 0.20
  # cal_exclude_from_val: true

  # Optional CAL from TEST (not recommended unless absolutely needed)
  # cal_from_test_frac: 0.20
  # cal_exclude_from_test: true

# -------------------------------
# MODEL
# -------------------------------
model:
  name: asnnet_secdual          # TCN + AttnPool dual encoders
  n_mels: 80
  emb: 128
  hidden_ch: 128
  dropout: 0.20
  heads: 1
  n_vocoders: 8
  grl_lambda: 0.2
  tcn_layers: 4
  tcn_dilations: [1, 2, 4, 8]

  # expose aux tokens/embeddings if your trainer uses CMRA/contrast
  return_aux: true

# -------------------------------
# TRAINING
# -------------------------------
train:
  epochs: 100
  batch_size: 256
  lr: 2.0e-4
  weight_decay: 1.0e-4
  grad_clip: 5.0
  seed: 42
  amp: false
  num_workers: 16
  prefetch_factor: 8
  persistent_workers: true
  pin_memory: true
  steps_per_epoch: null

  loss:
    name: balanced_bce
    use_effective_num: true
    effective_beta: 0.9999

    warmup:
      enable: true
      epochs: 5
      first: focal

    focal:
      gamma: 2.0
      alpha: 0.25

    asl:
      gamma_pos: 0.0
      gamma_neg: 4.0
      clip: 0.05

    logit_adjusted:
      tau: 1.0

  # Security-aware curriculum (gentle for WaveFake diversity)
  stability_warmup_frac: 0.20
  sam_start_frac:        0.40
  energy_start_frac:     0.50
  ood_start_frac:        0.60
  adv_start_frac:        0.70
  adv_warmup_epochs: 0

  lr_drop_on_switch: 0.30
  best_metric: eer
  best_mode: min
  save_every_epochs: 1
  keep_top_k: 3

# -------------------------------
# OPTIMIZER / SAM / SWA
# -------------------------------
optim:
  use_sam: true
  sam_rho: 0.002
  scheduler:
    name: cosine
    warmup_steps: 800
    min_lr: 1.0e-6
  swa:
    enable: true
    start_frac: 0.85           # NOTE: use start_frac (not start_epoch)
    update_bn_on_finish: true

ema:
  use_ema: true
  decay: 0.999

# -------------------------------
# AUGMENTATIONS
# -------------------------------
augs:
  use_specaug: true
  time_mask_T: 40
  time_mask_p: 0.40
  freq_mask_F: 8
  freq_mask_p: 0.40
  add_noise_p: 0.30
  snr_db: [10, 24]

# -------------------------------
# ASNET loss branch (optional CMRA/contrast hooks)
# -------------------------------
asn:
  use_trainer_asn: true
  win_ms: [80, 160]
  hop_ms: 40
  margin: 0.2
  spoof_weight: 0.5
  tv_weight: 0.0
  mel_sample: 16
  max_time_windows: 64

  # Coherence
  cons_weight: 0.05

  # === CMRA (keep lighter on WaveFake) ===
  cmra_weight: 0.04
  cmra_margin: 0.6
  cmra_band: [0.18, 0.45]
  cmra_token_weight: 0.20

  # Contrastive (dual-view)
  contrast_weight: 0.010
  contrast_tau: 0.070

  # Corridor shape
  cmra:
    s_align: 0.25
    s_max:   0.55
    w_align: 1.0
    w_repel: 0.6

# -------------------------------
# SECURITY & ROBUSTNESS
# -------------------------------
security:
  # Adversarial branch (late)
  use_adv: true
  adv_steps: 2
  adv_eps: [0.0005, 0.001, 0.0015, 0.002]
  adv_alpha: 0.0002
  adv_every: 10
  trades_weight: 0.0

  # Energy regularization (ID anchor)
  energy_reg: true
  energy_weight: 1.0e-5

  energy:
    tau_id: -0.10
    lambda_id: 1.0e-5
    warmup_epochs: 10

  # Gentle OOD push
  ood_push:
    use: true
    curriculum: true
    types: [mp3, bandstop, lowpass, reverb, bitrate16k]
    tau_target: 0.10
    weight: 5.0e-6

  # Energy triage & conformal abstention (eval)
  triage:
    T: 1.0
    tau_susp_energy: -0.25
    use_conformal: true
    target_coverage: 0.90
    auto_tau_eval: true
    tau_override: null

# -------------------------------
# EVALUATION
# -------------------------------
eval:
  use_test: false
  threshold: 0.50
  use_config_threshold: false
  csv_threshold_source: f1      # trainer-compatible default
  attack: "fgsm"
  save_pred_csv: true
  verbose: true
  use_temperature_scaling: true
  reliability_bins: 15
  ckpt_choose: best
